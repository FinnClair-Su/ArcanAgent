"""
Context Builder
Builds optimized context for LLM queries using bidirectional links and path analysis
Implements the core algorithm: find relevant notes -> shortest paths -> expand neighborhoods
"""

import logging
from typing import Dict, List, Set, Tuple, Optional, Any
from collections import defaultdict, deque
import heapq
from datetime import datetime, timedelta


logger = logging.getLogger(__name__)


class ContextBuilder:
    \"\"\"Builds optimal context graphs for LLM queries using bidirectional links\"\"\"\n    \n    def __init__(self, vault_manager, link_graph, path_finder):\n        self.vault = vault_manager\n        self.graph = link_graph\n        self.path_finder = path_finder\n        \n        # Context building parameters\n        self.max_context_size = 20\n        self.max_path_length = 5\n        self.neighborhood_radius = 1\n        self.relevance_threshold = 0.1\n        \n        # Scoring weights for context relevance\n        self.weights = {\n            \"query_match\": 3.0,\n            \"path_importance\": 2.0, \n            \"intersection_bonus\": 1.5,\n            \"recency_bonus\": 0.5,\n            \"link_density\": 1.0,\n            \"fsrs_maturity\": 0.8\n        }\n    \n    async def build_comprehensive_context(self, query: str, user_knowledge: List[str] = None) -> Dict[str, Any]:\n        \"\"\"Build comprehensive context for a query including user's knowledge state\"\"\"\n        \n        logger.info(f\"Building context for query: {query}\")\n        \n        # Step 1: Find directly relevant notes\n        primary_notes = await self._find_primary_notes(query)\n        \n        # Step 2: Include user's existing knowledge if provided\n        knowledge_notes = await self._include_user_knowledge(user_knowledge) if user_knowledge else []\n        \n        # Step 3: Find connecting paths between all relevant notes\n        all_seed_notes = list(set([n[\"note_id\"] for n in primary_notes] + knowledge_notes))\n        connection_paths = await self._find_connection_paths(all_seed_notes)\n        \n        # Step 4: Build the context graph\n        context_graph = await self._build_context_graph(\n            primary_notes, knowledge_notes, connection_paths\n        )\n        \n        # Step 5: Enhance with metadata and scoring\n        enhanced_context = await self._enhance_context_with_metadata(context_graph, query)\n        \n        # Step 6: Optimize context size and relevance\n        optimized_context = await self._optimize_context_selection(enhanced_context)\n        \n        return {\n            \"query\": query,\n            \"context_notes\": optimized_context,\n            \"total_notes\": len(optimized_context),\n            \"primary_notes\": len(primary_notes),\n            \"knowledge_notes\": len(knowledge_notes),\n            \"connection_paths\": connection_paths,\n            \"context_quality_score\": await self._calculate_context_quality(optimized_context)\n        }\n    \n    async def _find_primary_notes(self, query: str) -> List[Dict[str, Any]]:\n        \"\"\"Find notes directly relevant to the query\"\"\"\n        \n        # Use path_finder's relevance scoring\n        relevant_notes = await self.path_finder.find_relevant_notes(query, limit=8)\n        \n        primary_notes = []\n        for note_id, relevance_score in relevant_notes:\n            note_data = self.vault.note_index.get(note_id)\n            if note_data and relevance_score > self.relevance_threshold:\n                primary_notes.append({\n                    \"note_id\": note_id,\n                    \"relevance_score\": relevance_score,\n                    \"role\": \"primary\",\n                    \"title\": note_data.get(\"title\", \"\"),\n                    \"content_preview\": await self._generate_content_preview(note_data)\n                })\n        \n        return primary_notes\n    \n    async def _include_user_knowledge(self, user_knowledge: List[str]) -> List[str]:\n        \"\"\"Find notes corresponding to user's existing knowledge\"\"\"\n        \n        knowledge_note_ids = []\n        \n        for topic in user_knowledge:\n            relevant = await self.path_finder.find_relevant_notes(topic, limit=2)\n            for note_id, score in relevant:\n                if score > 0.5:  # High confidence match\n                    knowledge_note_ids.append(note_id)\n        \n        return list(set(knowledge_note_ids))\n    \n    async def _find_connection_paths(self, seed_notes: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Find shortest paths connecting all seed notes\"\"\"\n        \n        paths = []\n        \n        # Find paths between all pairs of seed notes\n        for i, source in enumerate(seed_notes):\n            for target in seed_notes[i+1:]:\n                shortest_path = await self.graph.find_shortest_path(source, target)\n                \n                if shortest_path and len(shortest_path) <= self.max_path_length:\n                    paths.append({\n                        \"source\": source,\n                        \"target\": target,\n                        \"path\": shortest_path,\n                        \"length\": len(shortest_path) - 1,\n                        \"bridging_notes\": shortest_path[1:-1],\n                        \"importance_score\": await self._calculate_path_importance(shortest_path)\n                    })\n        \n        # Sort paths by importance and length\n        paths.sort(key=lambda x: (x[\"importance_score\"], -x[\"length\"]), reverse=True)\n        \n        return paths[:10]  # Limit to top 10 paths\n    \n    async def _calculate_path_importance(self, path: List[str]) -> float:\n        \"\"\"Calculate importance score for a path\"\"\"\n        \n        if len(path) < 2:\n            return 0.0\n        \n        total_importance = 0.0\n        \n        for note_id in path:\n            # Get note importance from graph analysis\n            importance_data = await self.graph.analyze_note_importance(note_id)\n            pagerank_score = importance_data.get(\"pagerank\", 0.0)\n            degree_centrality = importance_data.get(\"degree_centrality\", 0.0)\n            \n            # Combine different importance measures\n            note_importance = (pagerank_score * 0.7) + (degree_centrality * 0.3)\n            total_importance += note_importance\n        \n        # Normalize by path length\n        return total_importance / len(path)\n    \n    async def _build_context_graph(self, primary_notes: List[Dict[str, Any]], \n                                 knowledge_notes: List[str], \n                                 connection_paths: List[Dict[str, Any]]) -> Set[str]:\n        \"\"\"Build the initial context graph from seeds and paths\"\"\"\n        \n        context_nodes = set()\n        \n        # Add all primary notes\n        for note in primary_notes:\n            context_nodes.add(note[\"note_id\"])\n        \n        # Add knowledge notes\n        context_nodes.update(knowledge_notes)\n        \n        # Add all notes from connection paths\n        for path_info in connection_paths:\n            context_nodes.update(path_info[\"path\"])\n        \n        # Find intersection points (nodes that appear in multiple paths)\n        intersection_points = self._find_path_intersections(connection_paths)\n        \n        # Expand around intersection points\n        for intersection in intersection_points[:3]:  # Top 3 intersections\n            if len(context_nodes) >= self.max_context_size:\n                break\n                \n            # Add neighbors around intersection points\n            neighbors = await self.graph.get_neighbors(\n                intersection, \n                direction=\"both\", \n                distance=self.neighborhood_radius\n            )\n            \n            # Add up to 2 neighbors per intersection\n            neighbor_count = 0\n            for neighbor in neighbors:\n                if (neighbor not in context_nodes and \n                    len(context_nodes) < self.max_context_size and \n                    neighbor_count < 2):\n                    \n                    context_nodes.add(neighbor)\n                    neighbor_count += 1\n        \n        return context_nodes\n    \n    def _find_path_intersections(self, paths: List[Dict[str, Any]]) -> List[str]:\n        \"\"\"Find nodes that appear in multiple paths (important intersections)\"\"\"\n        \n        node_frequency = defaultdict(int)\n        \n        for path_info in paths:\n            for node in path_info[\"path\"]:\n                node_frequency[node] += 1\n        \n        # Sort by frequency (most connected first)\n        intersections = [\n            node for node, freq in node_frequency.items() if freq > 1\n        ]\n        \n        intersections.sort(key=lambda x: node_frequency[x], reverse=True)\n        \n        return intersections\n    \n    async def _enhance_context_with_metadata(self, context_nodes: Set[str], query: str) -> List[Dict[str, Any]]:\n        \"\"\"Enhance context nodes with rich metadata for LLM consumption\"\"\"\n        \n        enhanced_context = []\n        query_lower = query.lower()\n        \n        for note_id in context_nodes:\n            note_data = self.vault.note_index.get(note_id)\n            if not note_data:\n                continue\n            \n            # Calculate various relevance scores\n            relevance_scores = await self._calculate_note_relevance(note_id, query_lower)\n            \n            # Get link information\n            outgoing_links = len(await self.graph.get_neighbors(note_id, direction=\"out\"))\n            incoming_links = len(await self.graph.get_neighbors(note_id, direction=\"in\"))\n            \n            # Get FSRS information if available\n            fsrs_data = note_data.get(\"frontmatter\", {}).get(\"fsrs\", {})\n            \n            enhanced_note = {\n                \"note_id\": note_id,\n                \"title\": note_data.get(\"title\", \"\"),\n                \"content\": note_data.get(\"content\", \"\"),\n                \"content_preview\": await self._generate_content_preview(note_data),\n                \"frontmatter\": note_data.get(\"frontmatter\", {}),\n                \n                # Relevance information\n                \"relevance_scores\": relevance_scores,\n                \"total_relevance\": sum(relevance_scores.values()),\n                \n                # Graph information\n                \"outgoing_links\": outgoing_links,\n                \"incoming_links\": incoming_links,\n                \"link_density\": (outgoing_links + incoming_links) / max(len(context_nodes) - 1, 1),\n                \n                # Learning information\n                \"fsrs_stability\": fsrs_data.get(\"stability\", 1.0),\n                \"fsrs_difficulty\": fsrs_data.get(\"difficulty\", 0.5),\n                \"is_due_for_review\": await self._is_due_for_review(fsrs_data),\n                \n                # Temporal information\n                \"created\": note_data.get(\"frontmatter\", {}).get(\"created\"),\n                \"updated\": note_data.get(\"frontmatter\", {}).get(\"updated\"),\n                \"recency_score\": await self._calculate_recency_score(note_data)\n            }\n            \n            enhanced_context.append(enhanced_note)\n        \n        return enhanced_context\n    \n    async def _calculate_note_relevance(self, note_id: str, query_lower: str) -> Dict[str, float]:\n        \"\"\"Calculate detailed relevance scores for a note\"\"\"\n        \n        note_data = self.vault.note_index.get(note_id)\n        if not note_data:\n            return {}\n        \n        scores = {}\n        \n        # Title matching\n        title = note_data.get(\"title\", \"\").lower()\n        if query_lower in title:\n            scores[\"title_match\"] = 2.0 if title == query_lower else 1.0\n        else:\n            scores[\"title_match\"] = 0.0\n        \n        # Content matching\n        content = note_data.get(\"content\", \"\").lower()\n        query_words = set(query_lower.split())\n        content_words = set(content.split())\n        word_overlap = len(query_words & content_words) / max(len(query_words), 1)\n        scores[\"content_match\"] = word_overlap\n        \n        # Tag matching\n        tags = set(tag.lower() for tag in note_data.get(\"frontmatter\", {}).get(\"tags\", []))\n        tag_overlap = len(query_words & tags) / max(len(query_words), 1)\n        scores[\"tag_match\"] = tag_overlap\n        \n        # Link-based relevance (how connected this note is)\n        importance_data = await self.graph.analyze_note_importance(note_id)\n        scores[\"graph_importance\"] = importance_data.get(\"pagerank\", 0.0)\n        \n        return scores\n    \n    async def _generate_content_preview(self, note_data: Dict[str, Any], max_words: int = 50) -> str:\n        \"\"\"Generate a preview of note content\"\"\"\n        \n        content = note_data.get(\"content\", \"\")\n        \n        # Remove markdown formatting for preview\n        import re\n        clean_content = re.sub(r'[#*`\\[\\]()_~]', '', content)\n        clean_content = re.sub(r'\\s+', ' ', clean_content).strip()\n        \n        words = clean_content.split()[:max_words]\n        preview = ' '.join(words)\n        \n        if len(clean_content.split()) > max_words:\n            preview += \"...\"\n        \n        return preview\n    \n    async def _is_due_for_review(self, fsrs_data: Dict[str, Any]) -> bool:\n        \"\"\"Check if note is due for FSRS review\"\"\"\n        \n        due_str = fsrs_data.get(\"due\")\n        if not due_str:\n            return False\n        \n        try:\n            due_date = datetime.fromisoformat(due_str.replace('Z', '+00:00'))\n            return due_date <= datetime.now()\n        except (ValueError, TypeError):\n            return False\n    \n    async def _calculate_recency_score(self, note_data: Dict[str, Any]) -> float:\n        \"\"\"Calculate recency score based on creation/update time\"\"\"\n        \n        updated_str = note_data.get(\"frontmatter\", {}).get(\"updated\")\n        if not updated_str:\n            return 0.0\n        \n        try:\n            updated_date = datetime.fromisoformat(updated_str.replace('Z', '+00:00'))\n            days_ago = (datetime.now() - updated_date).days\n            \n            # Exponential decay: more recent = higher score\n            recency_score = max(0.0, 1.0 - (days_ago / 30.0))  # 30-day window\n            return recency_score\n        except (ValueError, TypeError):\n            return 0.0\n    \n    async def _optimize_context_selection(self, enhanced_context: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Optimize context selection based on relevance and constraints\"\"\"\n        \n        # Calculate combined scores for each note\n        for note in enhanced_context:\n            relevance = note[\"total_relevance\"]\n            graph_importance = note[\"relevance_scores\"].get(\"graph_importance\", 0.0)\n            recency = note[\"recency_score\"]\n            link_density = note[\"link_density\"]\n            \n            # Weighted combination\n            combined_score = (\n                relevance * self.weights[\"query_match\"] +\n                graph_importance * self.weights[\"path_importance\"] +\n                recency * self.weights[\"recency_bonus\"] +\n                link_density * self.weights[\"link_density\"]\n            )\n            \n            note[\"combined_score\"] = combined_score\n        \n        # Sort by combined score\n        enhanced_context.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n        \n        # Ensure we include diverse types of notes\n        optimized = await self._ensure_context_diversity(enhanced_context)\n        \n        # Limit to max context size\n        return optimized[:self.max_context_size]\n    \n    async def _ensure_context_diversity(self, context_notes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Ensure diversity in context notes (primary, bridging, knowledge)\"\"\"\n        \n        # Categorize notes by role\n        primary_notes = [n for n in context_notes if n[\"relevance_scores\"].get(\"title_match\", 0) > 0.5]\n        bridging_notes = [n for n in context_notes if n[\"link_density\"] > 0.5]\n        knowledge_notes = [n for n in context_notes if n[\"fsrs_stability\"] > 2.0]  # Mature knowledge\n        \n        # Ensure representation from each category\n        diverse_context = []\n        \n        # Add top primary notes\n        diverse_context.extend(primary_notes[:6])\n        \n        # Add top bridging notes (not already included)\n        for note in bridging_notes:\n            if note not in diverse_context and len(diverse_context) < self.max_context_size:\n                diverse_context.append(note)\n            if len(diverse_context) >= self.max_context_size:\n                break\n        \n        # Fill remaining slots with highest scored notes\n        for note in context_notes:\n            if note not in diverse_context and len(diverse_context) < self.max_context_size:\n                diverse_context.append(note)\n        \n        return diverse_context\n    \n    async def _calculate_context_quality(self, context_notes: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate overall quality score for the context\"\"\"\n        \n        if not context_notes:\n            return 0.0\n        \n        # Average relevance\n        avg_relevance = sum(note[\"total_relevance\"] for note in context_notes) / len(context_notes)\n        \n        # Connectivity (how well connected the notes are)\n        total_links = sum(note[\"incoming_links\"] + note[\"outgoing_links\"] for note in context_notes)\n        connectivity = total_links / len(context_notes)\n        \n        # Diversity (different types of content)\n        unique_tags = set()\n        for note in context_notes:\n            tags = note[\"frontmatter\"].get(\"tags\", [])\n            unique_tags.update(tags)\n        diversity = len(unique_tags) / max(len(context_notes), 1)\n        \n        # Combined quality score\n        quality_score = (\n            avg_relevance * 0.5 +\n            min(connectivity / 5.0, 1.0) * 0.3 +  # Normalize connectivity\n            min(diversity, 1.0) * 0.2\n        )\n        \n        return min(quality_score, 1.0)\n    \n    async def build_learning_context(self, topic: str, user_level: str = \"beginner\") -> Dict[str, Any]:\n        \"\"\"Build context specifically optimized for learning about a topic\"\"\"\n        \n        # Find notes about the topic\n        topic_notes = await self.path_finder.find_relevant_notes(topic, limit=5)\n        \n        if not topic_notes:\n            return {\"error\": \"No notes found for topic\"}\n        \n        main_note_id = topic_notes[0][0]\n        \n        # Get prerequisites and follow-up topics\n        prerequisites = await self.path_finder._get_prerequisites(main_note_id)\n        next_topics = await self.path_finder.suggest_next_topics([main_note_id], limit=3)\n        \n        # Build learning-optimized context\n        learning_context = {\n            \"topic\": topic,\n            \"user_level\": user_level,\n            \"main_concept\": await self._get_note_summary(main_note_id),\n            \"prerequisites\": [await self._get_note_summary(pid) for pid in prerequisites],\n            \"next_topics\": [{\n                \"summary\": await self._get_note_summary(nt[\"note_id\"]),\n                \"readiness_score\": nt[\"readiness_score\"]\n            } for nt in next_topics],\n            \"difficulty_progression\": await self._analyze_difficulty_progression(\n                prerequisites + [main_note_id] + [nt[\"note_id\"] for nt in next_topics]\n            )\n        }\n        \n        return learning_context\n    \n    async def _get_note_summary(self, note_id: str) -> Dict[str, Any]:\n        \"\"\"Get a summary of a note for learning context\"\"\"\n        \n        note_data = self.vault.note_index.get(note_id)\n        if not note_data:\n            return {}\n        \n        return {\n            \"note_id\": note_id,\n            \"title\": note_data.get(\"title\", \"\"),\n            \"preview\": await self._generate_content_preview(note_data, max_words=30),\n            \"difficulty\": note_data.get(\"frontmatter\", {}).get(\"difficulty\", 0.5),\n            \"tags\": note_data.get(\"frontmatter\", {}).get(\"tags\", [])\n        }\n    \n    async def _analyze_difficulty_progression(self, note_ids: List[str]) -> List[float]:\n        \"\"\"Analyze difficulty progression through a sequence of notes\"\"\"\n        \n        difficulties = []\n        \n        for note_id in note_ids:\n            note_data = self.vault.note_index.get(note_id)\n            if note_data:\n                difficulty = note_data.get(\"frontmatter\", {}).get(\"difficulty\", 0.5)\n                difficulties.append(difficulty)\n        \n        return difficulties"