"""
Path Finder
Finds optimal paths between notes and builds context graphs for queries
"""

import logging
from typing import Dict, List, Set, Tuple, Optional, Any
from collections import defaultdict, deque
import heapq
from itertools import combinations


logger = logging.getLogger(__name__)


class PathFinder:
    \"\"\"Finds paths between notes and builds context for LLM queries\"\"\"\n    \n    def __init__(self, link_graph, vault_manager):\n        self.graph = link_graph\n        self.vault = vault_manager\n        \n        # Scoring weights for different matching criteria\n        self.weights = {\n            \"exact_title_match\": 2.0,\n            \"partial_title_match\": 1.5,\n            \"content_match\": 1.0,\n            \"tag_match\": 0.8,\n            \"recent_note\": 0.5\n        }\n    \n    async def find_relevant_notes(self, query: str, limit: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"Find notes relevant to a query with relevance scores\"\"\"\n        \n        query_lower = query.lower()\n        query_words = set(query_lower.split())\n        \n        scored_notes = []\n        \n        for note_id, note_data in self.vault.note_index.items():\n            score = 0.0\n            \n            # Title matching\n            title = note_data.get(\"title\", \"\").lower()\n            if title == query_lower:\n                score += self.weights[\"exact_title_match\"]\n            elif query_lower in title or any(word in title for word in query_words):\n                score += self.weights[\"partial_title_match\"]\n            \n            # Content matching\n            content = note_data.get(\"content\", \"\").lower()\n            content_words = set(content.split())\n            word_overlap = len(query_words & content_words) / max(len(query_words), 1)\n            score += word_overlap * self.weights[\"content_match\"]\n            \n            # Tag matching\n            tags = set(tag.lower() for tag in note_data.get(\"frontmatter\", {}).get(\"tags\", []))\n            if query_words & tags:\n                score += self.weights[\"tag_match\"]\n            \n            # Boost recent notes slightly\n            updated = note_data.get(\"frontmatter\", {}).get(\"updated\")\n            if updated:\n                # Simple recency boost (could be more sophisticated)\n                score += self.weights[\"recent_note\"] * 0.1\n            \n            if score > 0:\n                scored_notes.append((note_id, score))\n        \n        # Sort by score and return top results\n        scored_notes.sort(key=lambda x: x[1], reverse=True)\n        return scored_notes[:limit]\n    \n    async def build_query_context(self, query: str, max_context_size: int = 15) -> Dict[str, Any]:\n        \"\"\"Build comprehensive context for a query including paths and neighborhoods\"\"\"\n        \n        # Find relevant notes\n        relevant_notes = await self.find_relevant_notes(query, limit=8)\n        \n        if not relevant_notes:\n            return {\"context_notes\": [], \"paths\": [], \"query\": query}\n        \n        # Extract note IDs\n        note_ids = [note_id for note_id, score in relevant_notes]\n        \n        # Find paths between all pairs of relevant notes\n        paths = await self._find_connecting_paths(note_ids)\n        \n        # Build comprehensive context graph\n        context_graph = await self._build_context_graph(\n            note_ids, paths, max_context_size\n        )\n        \n        # Get context notes with metadata\n        context_notes = []\n        for note_id in context_graph:\n            note_data = self.vault.note_index.get(note_id)\n            if note_data:\n                # Calculate relevance to original query\n                relevance = next(\n                    (score for nid, score in relevant_notes if nid == note_id), \n                    0.0\n                )\n                \n                context_notes.append({\n                    \"note_id\": note_id,\n                    \"title\": note_data.get(\"title\", \"\"),\n                    \"content\": note_data.get(\"content\", \"\"),\n                    \"frontmatter\": note_data.get(\"frontmatter\", {}),\n                    \"relevance_score\": relevance,\n                    \"is_primary\": note_id in note_ids,\n                    \"context_role\": \"primary\" if note_id in note_ids else \"bridging\"\n                })\n        \n        return {\n            \"context_notes\": context_notes,\n            \"paths\": paths,\n            \"query\": query,\n            \"total_context_size\": len(context_notes),\n            \"primary_notes\": len(note_ids)\n        }\n    \n    async def _find_connecting_paths(self, note_ids: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Find shortest paths between all pairs of relevant notes\"\"\"\n        \n        paths = []\n        \n        # Find paths between all pairs\n        for i, source in enumerate(note_ids):\n            for target in note_ids[i+1:]:\n                path = await self.graph.find_shortest_path(source, target)\n                \n                if path and len(path) > 1:  # Valid path exists\n                    paths.append({\n                        \"source\": source,\n                        \"target\": target,\n                        \"path\": path,\n                        \"length\": len(path) - 1,\n                        \"bridging_notes\": path[1:-1]  # Intermediate notes\n                    })\n        \n        # Sort paths by length (prefer shorter paths)\n        paths.sort(key=lambda x: x[\"length\"])\n        \n        return paths\n    \n    async def _build_context_graph(self, primary_notes: List[str], paths: List[Dict[str, Any]], max_size: int) -> Set[str]:\n        \"\"\"Build the final context graph including primary notes, paths, and neighborhoods\"\"\"\n        \n        context_nodes = set(primary_notes)\n        \n        # Add all notes from paths\n        for path_info in paths:\n            context_nodes.update(path_info[\"path\"])\n            \n            # Stop if we're getting too large\n            if len(context_nodes) >= max_size:\n                break\n        \n        # Add neighbors around key intersection points\n        intersection_points = self._find_intersection_points(paths)\n        \n        for node in intersection_points:\n            if len(context_nodes) >= max_size:\n                break\n                \n            # Add immediate neighbors\n            neighbors = await self.graph.get_neighbors(node, direction=\"both\", distance=1)\n            \n            # Add up to 2 neighbors per intersection point\n            neighbor_count = 0\n            for neighbor in neighbors:\n                if neighbor not in context_nodes and len(context_nodes) < max_size:\n                    context_nodes.add(neighbor)\n                    neighbor_count += 1\n                    if neighbor_count >= 2:\n                        break\n        \n        return context_nodes\n    \n    def _find_intersection_points(self, paths: List[Dict[str, Any]]) -> List[str]:\n        \"\"\"Find notes that appear in multiple paths (important intersection points)\"\"\"\n        \n        node_frequency = defaultdict(int)\n        \n        for path_info in paths:\n            for node in path_info[\"path\"]:\n                node_frequency[node] += 1\n        \n        # Sort by frequency and return nodes that appear in multiple paths\n        intersections = [\n            node for node, freq in node_frequency.items() if freq > 1\n        ]\n        \n        intersections.sort(key=lambda x: node_frequency[x], reverse=True)\n        \n        return intersections\n    \n    async def find_learning_path(self, start_concept: str, target_concept: str) -> Dict[str, Any]:\n        \"\"\"Find an optimal learning path between two concepts\"\"\"\n        \n        # Find notes for start and target concepts\n        start_notes = await self.find_relevant_notes(start_concept, limit=3)\n        target_notes = await self.find_relevant_notes(target_concept, limit=3)\n        \n        if not start_notes or not target_notes:\n            return {\"path\": None, \"error\": \"Could not find relevant notes\"}\n        \n        start_id = start_notes[0][0]\n        target_id = target_notes[0][0]\n        \n        # Find shortest path\n        path = await self.graph.find_shortest_path(start_id, target_id)\n        \n        if not path:\n            return {\"path\": None, \"error\": \"No path found between concepts\"}\n        \n        # Enhance path with metadata\n        enhanced_path = []\n        \n        for i, note_id in enumerate(path):\n            note_data = self.vault.note_index.get(note_id)\n            if note_data:\n                \n                # Analyze difficulty progression\n                difficulty = note_data.get(\"frontmatter\", {}).get(\"difficulty\", 0.5)\n                \n                # Calculate prerequisites (notes that should be learned before this one)\n                prerequisites = await self._get_prerequisites(note_id)\n                \n                enhanced_path.append({\n                    \"note_id\": note_id,\n                    \"title\": note_data.get(\"title\", \"\"),\n                    \"difficulty\": difficulty,\n                    \"prerequisites\": prerequisites,\n                    \"position\": i,\n                    \"is_start\": i == 0,\n                    \"is_end\": i == len(path) - 1\n                })\n        \n        return {\n            \"path\": enhanced_path,\n            \"total_steps\": len(path),\n            \"start_concept\": start_concept,\n            \"target_concept\": target_concept,\n            \"estimated_difficulty\": sum(step[\"difficulty\"] for step in enhanced_path) / len(enhanced_path)\n        }\n    \n    async def _get_prerequisites(self, note_id: str) -> List[str]:\n        \"\"\"Get prerequisite notes for a given note\"\"\"\n        \n        # Notes that link TO this note can be considered prerequisites\n        prerequisites = list(await self.graph.get_neighbors(note_id, direction=\"in\", distance=1))\n        \n        # Filter by difficulty (prerequisites should be easier)\n        current_difficulty = self.vault.note_index.get(note_id, {}).get(\"frontmatter\", {}).get(\"difficulty\", 0.5)\n        \n        filtered_prerequisites = []\n        for prereq_id in prerequisites:\n            prereq_data = self.vault.note_index.get(prereq_id)\n            if prereq_data:\n                prereq_difficulty = prereq_data.get(\"frontmatter\", {}).get(\"difficulty\", 0.5)\n                if prereq_difficulty <= current_difficulty:\n                    filtered_prerequisites.append(prereq_id)\n        \n        return filtered_prerequisites\n    \n    async def suggest_next_topics(self, current_knowledge: List[str], limit: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Suggest next topics to learn based on current knowledge\"\"\"\n        \n        suggestions = []\n        current_note_ids = []\n        \n        # Find notes for current knowledge\n        for topic in current_knowledge:\n            relevant = await self.find_relevant_notes(topic, limit=2)\n            current_note_ids.extend([note_id for note_id, score in relevant])\n        \n        current_note_ids = list(set(current_note_ids))  # Remove duplicates\n        \n        # Find notes that are linked from current knowledge but not yet known\n        candidate_notes = set()\n        \n        for note_id in current_note_ids:\n            # Get outgoing links (topics these lead to)\n            outgoing = await self.graph.get_neighbors(note_id, direction=\"out\", distance=1)\n            candidate_notes.update(outgoing)\n        \n        # Remove already known topics\n        candidate_notes -= set(current_note_ids)\n        \n        # Score candidates based on multiple factors\n        for candidate_id in candidate_notes:\n            note_data = self.vault.note_index.get(candidate_id)\n            if not note_data:\n                continue\n            \n            # Calculate readiness score\n            readiness = await self._calculate_readiness_score(candidate_id, current_note_ids)\n            \n            # Calculate importance score\n            importance = await self.graph.analyze_note_importance(candidate_id)\n            importance_score = importance.get(\"pagerank\", 0)\n            \n            # Combine scores\n            total_score = readiness * 0.7 + importance_score * 0.3\n            \n            suggestions.append({\n                \"note_id\": candidate_id,\n                \"title\": note_data.get(\"title\", \"\"),\n                \"readiness_score\": readiness,\n                \"importance_score\": importance_score,\n                \"total_score\": total_score,\n                \"difficulty\": note_data.get(\"frontmatter\", {}).get(\"difficulty\", 0.5)\n            })\n        \n        # Sort by total score and return top suggestions\n        suggestions.sort(key=lambda x: x[\"total_score\"], reverse=True)\n        \n        return suggestions[:limit]\n    \n    async def _calculate_readiness_score(self, candidate_id: str, known_note_ids: List[str]) -> float:\n        \"\"\"Calculate how ready a user is to learn a topic based on prerequisites\"\"\"\n        \n        # Get prerequisites for the candidate topic\n        prerequisites = await self._get_prerequisites(candidate_id)\n        \n        if not prerequisites:\n            return 1.0  # No prerequisites, fully ready\n        \n        # Calculate what percentage of prerequisites are already known\n        known_prerequisites = len(set(prerequisites) & set(known_note_ids))\n        readiness = known_prerequisites / len(prerequisites)\n        \n        return readiness\n    \n    async def analyze_knowledge_gaps(self, user_knowledge: List[str]) -> Dict[str, Any]:\n        \"\"\"Analyze gaps in user's knowledge and suggest filling them\"\"\"\n        \n        # Find user's current knowledge notes\n        known_note_ids = []\n        for topic in user_knowledge:\n            relevant = await self.find_relevant_notes(topic, limit=2)\n            known_note_ids.extend([note_id for note_id, score in relevant])\n        \n        known_note_ids = list(set(known_note_ids))\n        \n        # Find clusters around known notes\n        knowledge_clusters = []\n        for note_id in known_note_ids:\n            cluster = await self.graph.get_cluster_around_note(note_id, max_size=10)\n            knowledge_clusters.append(cluster)\n        \n        # Find gaps (important notes in clusters that user doesn't know)\n        all_cluster_notes = set()\n        for cluster in knowledge_clusters:\n            all_cluster_notes.update(cluster)\n        \n        gap_notes = all_cluster_notes - set(known_note_ids)\n        \n        # Score gaps by importance\n        gaps = []\n        for gap_id in gap_notes:\n            note_data = self.vault.note_index.get(gap_id)\n            if note_data:\n                importance = await self.graph.analyze_note_importance(gap_id)\n                \n                gaps.append({\n                    \"note_id\": gap_id,\n                    \"title\": note_data.get(\"title\", \"\"),\n                    \"importance\": importance.get(\"pagerank\", 0),\n                    \"connections\": importance.get(\"total_degree\", 0)\n                })\n        \n        gaps.sort(key=lambda x: x[\"importance\"], reverse=True)\n        \n        return {\n            \"known_topics\": len(known_note_ids),\n            \"potential_gaps\": len(gaps),\n            \"top_gaps\": gaps[:10],\n            \"knowledge_clusters\": len(knowledge_clusters)\n        }"